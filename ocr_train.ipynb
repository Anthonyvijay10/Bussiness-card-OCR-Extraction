{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hczeW9lf3bp_",
        "outputId": "d43f893e-7044-4999-bbcd-ca8a73f3e27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: paddlepaddle in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (9.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.20.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle) (1.2.2)\n",
            "Requirement already satisfied: paddlepaddle-gpu in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (9.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (4.4.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.20.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle-gpu) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle-gpu) (1.2.2)\n",
            "Requirement already satisfied: paddleocr in /usr/local/lib/python3.10/dist-packages (2.8.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.0.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.23.2)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.4.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.3.0.post5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.66.4)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.9.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.0.10)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddleocr) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from paddleocr) (6.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.12.3)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.53.1)\n",
            "Requirement already satisfied: fire>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->paddleocr) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (3.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (2.34.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (3.3)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2024.7.21)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (0.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->paddleocr) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->paddleocr) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (2024.7.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (2.8.2)\n",
            "fatal: You must specify a repository to clone.\n",
            "\n",
            "usage: git clone [<options>] [--] <repo> [<dir>]\n",
            "\n",
            "    -v, --verbose         be more verbose\n",
            "    -q, --quiet           be more quiet\n",
            "    --progress            force progress reporting\n",
            "    --reject-shallow      don't clone shallow repository\n",
            "    -n, --no-checkout     don't create a checkout\n",
            "    --bare                create a bare repository\n",
            "    --mirror              create a mirror repository (implies bare)\n",
            "    -l, --local           to clone from a local repository\n",
            "    --no-hardlinks        don't use local hardlinks, always copy\n",
            "    -s, --shared          setup as shared repository\n",
            "    --recurse-submodules[=<pathspec>]\n",
            "                          initialize submodules in the clone\n",
            "    --recursive ...       alias of --recurse-submodules\n",
            "    -j, --jobs <n>        number of submodules cloned in parallel\n",
            "    --template <template-directory>\n",
            "                          directory from which templates will be used\n",
            "    --reference <repo>    reference repository\n",
            "    --reference-if-able <repo>\n",
            "                          reference repository\n",
            "    --dissociate          use --reference only while cloning\n",
            "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
            "    -b, --branch <branch>\n",
            "                          checkout <branch> instead of the remote's HEAD\n",
            "    -u, --upload-pack <path>\n",
            "                          path to git-upload-pack on the remote\n",
            "    --depth <depth>       create a shallow clone of that depth\n",
            "    --shallow-since <time>\n",
            "                          create a shallow clone since a specific time\n",
            "    --shallow-exclude <revision>\n",
            "                          deepen history of shallow clone, excluding rev\n",
            "    --single-branch       clone only one branch, HEAD or --branch\n",
            "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
            "    --shallow-submodules  any cloned submodules will be shallow\n",
            "    --separate-git-dir <gitdir>\n",
            "                          separate git dir from working tree\n",
            "    -c, --config <key=value>\n",
            "                          set config inside the new repository\n",
            "    --server-option <server-specific>\n",
            "                          option to transmit\n",
            "    -4, --ipv4            use IPv4 addresses only\n",
            "    -6, --ipv6            use IPv6 addresses only\n",
            "    --filter <args>       object filtering\n",
            "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
            "    --sparse              initialize sparse-checkout file to include only files at root\n",
            "\n",
            "[2024/07/27 10:27:27] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
            "[2024/07/27 10:27:28] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n",
            "[2024/07/27 10:27:28] ppocr DEBUG: dt_boxes num : 10, elapsed : 0.08420681953430176\n",
            "[2024/07/27 10:27:30] ppocr DEBUG: rec_res num  : 10, elapsed : 2.001398801803589\n",
            "cognizant intuition engineered Tanmay Chakraborty Technical Leadership tanmay.chakraborty@cognizant.com A-4.Nandini Apartment Near Chinar Park tanmay.chakraborty2003@.gmail.com Kolkata-700059India +919748076379\n",
            "OCR output has been written to: C:/Users/Vishal/Desktop/out_dir/ocr_output.txt\n"
          ]
        }
      ],
      "source": [
        "!pip install paddlepaddle\n",
        "!pip install paddlepaddle-gpu\n",
        "!pip install paddleocr\n",
        "import paddleocr\n",
        "!git clone\n",
        "from paddleocr import PaddleOCR, draw_ocr # main OCR dependencies\n",
        "import cv2 #opencv\n",
        "import os # folder directoryÂ navigation\n",
        "\n",
        "ocr_model = PaddleOCR(lang='en')\n",
        "img_path =\"/cognizant.jpg\"\n",
        "\n",
        "# Perform OCR on the image\n",
        "result = ocr_model.ocr(img_path)\n",
        "\n",
        "# Print the entire OCR result\n",
        "for line in result:\n",
        "    print(' '.join([word_info[1][0] for word_info in line]))\n",
        "    import os\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = \"C:/Users/Vishal/Desktop/out_dir\"\n",
        "\n",
        "# Check if the output directory exists, if not create it\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define the path for the output text file\n",
        "output_text_file = os.path.join(output_dir, 'ocr_output.txt')\n",
        "\n",
        "# Extract text from OCR result\n",
        "ocr_text = \"\"\n",
        "for line in result:\n",
        "    ocr_text += ' '.join([word_info[1][0] for word_info in line]) + '\\n'\n",
        "\n",
        "# Write the extracted text into a text file\n",
        "with open(output_text_file, 'w') as f:\n",
        "    f.write(ocr_text)\n",
        "\n",
        "print(\"OCR output has been written to:\", output_text_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# Load spaCy model for organization names\n",
        "nlp_org = spacy.blank(\"en\")\n",
        "\n",
        "# Define the NER pipeline component for organization names\n",
        "ner_org = nlp_org.add_pipe(\"ner\")\n",
        "ner_org.add_label(\"ORG\")\n",
        "\n",
        "# Read data from CSV file for organization names\n",
        "df_org = pd.read_csv(\"/content/Company Names.csv\")  # Update with your CSV file\n",
        "\n",
        "# Convert data into spaCy training format for organization names\n",
        "train_examples_org = []\n",
        "for _, row in df_org.iterrows():\n",
        "    # Use organization name as the text\n",
        "    text = row[\"Company Name\"]  # Update with the correct column name\n",
        "    # Add label for the entire text\n",
        "    annotations = {\"entities\": [(0, len(text), \"ORG\")]}\n",
        "    example = Example.from_dict(nlp_org.make_doc(text), annotations)\n",
        "    train_examples_org.append(example)\n",
        "\n",
        "# Training loop for organization names\n",
        "other_pipes_org = [pipe for pipe in nlp_org.pipe_names if pipe != \"ner\"]\n",
        "with nlp_org.disable_pipes(*other_pipes_org):\n",
        "    optimizer_org = nlp_org.begin_training()\n",
        "    for iteration in range(50):\n",
        "        losses_org = {}\n",
        "        random.shuffle(train_examples_org)\n",
        "        for example in train_examples_org:\n",
        "            nlp_org.update([example], drop=0.5, losses=losses_org)\n",
        "        print(\"Organization Names - Iteration\", iteration, \"Losses\", losses_org)\n",
        "\n",
        "# Save the trained model for organization names\n",
        "nlp_org.to_disk(\"ner_model_org\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuYvz76YZhfm",
        "outputId": "090cf68e-841f-408a-a9d5-ec0a32c879b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organization Names - Iteration 0 Losses {'ner': 596.8187235730832}\n",
            "Organization Names - Iteration 1 Losses {'ner': 42.83886153970551}\n",
            "Organization Names - Iteration 2 Losses {'ner': 19.120286726433736}\n",
            "Organization Names - Iteration 3 Losses {'ner': 5.5143368921603395}\n",
            "Organization Names - Iteration 4 Losses {'ner': 31.263607005621896}\n",
            "Organization Names - Iteration 5 Losses {'ner': 9.788759623909136e-09}\n",
            "Organization Names - Iteration 6 Losses {'ner': 8.785343626350159}\n",
            "Organization Names - Iteration 7 Losses {'ner': 23.10561631982133}\n",
            "Organization Names - Iteration 8 Losses {'ner': 9.95975898797382}\n",
            "Organization Names - Iteration 9 Losses {'ner': 13.96748114247724}\n",
            "Organization Names - Iteration 10 Losses {'ner': 16.058507174906264}\n",
            "Organization Names - Iteration 11 Losses {'ner': 15.10430126668428}\n",
            "Organization Names - Iteration 12 Losses {'ner': 6.996585023794156}\n",
            "Organization Names - Iteration 13 Losses {'ner': 9.433614391593765e-09}\n",
            "Organization Names - Iteration 14 Losses {'ner': 4.254542971953003}\n",
            "Organization Names - Iteration 15 Losses {'ner': 2.1048730613921784}\n",
            "Organization Names - Iteration 16 Losses {'ner': 17.525116590979277}\n",
            "Organization Names - Iteration 17 Losses {'ner': 1.4934825147563282e-10}\n",
            "Organization Names - Iteration 18 Losses {'ner': 13.715771893669778}\n",
            "Organization Names - Iteration 19 Losses {'ner': 1.9985766380297942}\n",
            "Organization Names - Iteration 20 Losses {'ner': 26.60304564016359}\n",
            "Organization Names - Iteration 21 Losses {'ner': 6.678898413917153e-08}\n",
            "Organization Names - Iteration 22 Losses {'ner': 1.3603450804055128e-12}\n",
            "Organization Names - Iteration 23 Losses {'ner': 14.445402778714955}\n",
            "Organization Names - Iteration 24 Losses {'ner': 11.503280308068879}\n",
            "Organization Names - Iteration 25 Losses {'ner': 6.030520784307814}\n",
            "Organization Names - Iteration 26 Losses {'ner': 4.378708172519317e-09}\n",
            "Organization Names - Iteration 27 Losses {'ner': 8.117393150956508e-13}\n",
            "Organization Names - Iteration 28 Losses {'ner': 7.456703360487203e-15}\n",
            "Organization Names - Iteration 29 Losses {'ner': 7.2312760406933325}\n",
            "Organization Names - Iteration 30 Losses {'ner': 18.05401332519485}\n",
            "Organization Names - Iteration 31 Losses {'ner': 2.7967902232621025e-09}\n",
            "Organization Names - Iteration 32 Losses {'ner': 3.80717422627571}\n",
            "Organization Names - Iteration 33 Losses {'ner': 6.039116816317759}\n",
            "Organization Names - Iteration 34 Losses {'ner': 30.093162739370882}\n",
            "Organization Names - Iteration 35 Losses {'ner': 2.1374934012018143e-07}\n",
            "Organization Names - Iteration 36 Losses {'ner': 2.1114458032048025e-12}\n",
            "Organization Names - Iteration 37 Losses {'ner': 3.629796290940728e-12}\n",
            "Organization Names - Iteration 38 Losses {'ner': 11.693538981910429}\n",
            "Organization Names - Iteration 39 Losses {'ner': 3.187260729987377e-08}\n",
            "Organization Names - Iteration 40 Losses {'ner': 5.1380519166515215e-11}\n",
            "Organization Names - Iteration 41 Losses {'ner': 3.4563760883354805e-13}\n",
            "Organization Names - Iteration 42 Losses {'ner': 2.5077958956150277e-15}\n",
            "Organization Names - Iteration 43 Losses {'ner': 12.104763926991017}\n",
            "Organization Names - Iteration 44 Losses {'ner': 3.9962534240822454e-07}\n",
            "Organization Names - Iteration 45 Losses {'ner': 2.1669068326592044e-11}\n",
            "Organization Names - Iteration 46 Losses {'ner': 33.510731918113954}\n",
            "Organization Names - Iteration 47 Losses {'ner': 4.026339810321163e-07}\n",
            "Organization Names - Iteration 48 Losses {'ner': 9.963320329671871e-12}\n",
            "Organization Names - Iteration 49 Losses {'ner': 2.7302590671423995e-14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model for organization names\n",
        "model_path = \"/content/ner_model_org\"\n",
        "nlp_org.to_disk(model_path)\n",
        "\n",
        "print(\"Model saved to:\", model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U2DAN_fIjEx",
        "outputId": "a1afe909-d64b-4d6a-c197-92e03f96f7bf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/ner_model_org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the model directory\n",
        "!zip -r /content/ner_model_org.zip /content/ner_model_org\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaX_H8JVJj9U",
        "outputId": "b87673c6-01c7-4b02-d07b-ad38878d5390"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ner_model_org/ (stored 0%)\n",
            "  adding: content/ner_model_org/meta.json (deflated 49%)\n",
            "  adding: content/ner_model_org/ner/ (stored 0%)\n",
            "  adding: content/ner_model_org/ner/moves (deflated 38%)\n",
            "  adding: content/ner_model_org/ner/cfg (deflated 33%)\n",
            "  adding: content/ner_model_org/ner/model (deflated 8%)\n",
            "  adding: content/ner_model_org/tokenizer (deflated 81%)\n",
            "  adding: content/ner_model_org/config.cfg (deflated 59%)\n",
            "  adding: content/ner_model_org/vocab/ (stored 0%)\n",
            "  adding: content/ner_model_org/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/ner_model_org/vocab/strings.json (deflated 72%)\n",
            "  adding: content/ner_model_org/vocab/vectors (deflated 45%)\n",
            "  adding: content/ner_model_org/vocab/key2row (stored 0%)\n",
            "  adding: content/ner_model_org/vocab/lookups.bin (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the zipped model directory\n",
        "files.download(\"/content/ner_model_org.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KjwGym3VJsoY",
        "outputId": "783d0ad5-8bf8-4fed-96f7-37313e3ca3bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7b14f86a-ee99-4e06-bd66-aebd3e4837f9\", \"ner_model_org.zip\", 3624687)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def extract_organization_and_person_names(text):\n",
        "    # Extract company name using regex (assuming it appears in the email domain)\n",
        "    organization_match = re.search(r'@([A-Za-z]+)\\.com', text)\n",
        "    organization_name = organization_match.group(1).capitalize() if organization_match else \"\"\n",
        "\n",
        "    # Extract person name using regex (assuming it appears before the email)\n",
        "    person_name_match = re.search(r'([A-Za-z\\.]+)@', text)\n",
        "    person_name = person_name_match.group(1).replace('.', ' ').strip().title() if person_name_match else \"\"\n",
        "\n",
        "    return organization_name, person_name\n",
        "\n",
        "# Take input from the user\n",
        "text = input(\"Enter the text for extraction: \")\n",
        "\n",
        "# Extract company name and person name\n",
        "organization_name, person_name = extract_organization_and_person_names(text)\n",
        "\n",
        "# Specify the directories where you want to save the files\n",
        "organization_dir = \"/var/organization\"\n",
        "person_dir = \"/var/person\"\n",
        "\n",
        "# Ensure directories exist, create if not\n",
        "os.makedirs(organization_dir, exist_ok=True)\n",
        "os.makedirs(person_dir, exist_ok=True)\n",
        "\n",
        "# Write the extracted organization name to a text file\n",
        "organization_output_filename = \"organization_name.txt\"\n",
        "organization_output_path = os.path.join(organization_dir, organization_output_filename)\n",
        "with open(organization_output_path, \"w\") as file:\n",
        "    file.write(organization_name)\n",
        "\n",
        "print(f\"Organization name has been written to '{organization_output_path}'.\")\n",
        "\n",
        "# Write the extracted person name to a text file\n",
        "person_output_filename = \"person_name.txt\"\n",
        "person_output_path = os.path.join(person_dir, person_output_filename)\n",
        "with open(person_output_path, \"w\") as file:\n",
        "    file.write(person_name)\n",
        "\n",
        "print(f\"Person name has been written to '{person_output_path}'.\")\n",
        "\n",
        "# Print the extracted names as output\n",
        "print(f\"Extracted Organization Name: {organization_name}\")\n",
        "print(f\"Person Name: {person_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Hb1IGIgTMu",
        "outputId": "5d0f8ce9-4b61-4e7f-8e64-6727c469fba2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text for extraction: cognizant intuition engineered Tanmay Chakraborty Technical Leadership tanmay.chakraborty@cognizant.com A-4.Nandini Apartment Near Chinar Park tanmay.chakraborty2003@.gmail.com Kolkata-700059India +919748076379\n",
            "Organization name has been written to '/var/organization/organization_name.txt'.\n",
            "Person name has been written to '/var/person/person_name.txt'.\n",
            "Extracted Organization Name: Cognizant\n",
            " Person Name: Tanmay Chakraborty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "import streamlit as st\n",
        "import os\n",
        "import re\n",
        "import paddleocr\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# Initialize PaddleOCR\n",
        "ocr_model = paddleocr.PaddleOCR(lang='en')\n",
        "\n",
        "def extract_organization_and_person_names(text):\n",
        "    # Extract company name using regex (assuming it appears in the email domain)\n",
        "    organization_match = re.search(r'@([A-Za-z]+)\\.com', text)\n",
        "    organization_name = organization_match.group(1).capitalize() if organization_match else \"Not Found\"\n",
        "\n",
        "    # Extract person name using regex (assuming it appears before the email)\n",
        "    person_name_match = re.search(r'([A-Za-z\\.]+)@', text)\n",
        "    person_name = person_name_match.group(1).replace('.', ' ').strip().title() if person_name_match else \"Not Found\"\n",
        "\n",
        "    return organization_name, person_name\n",
        "\n",
        "def ocr_image(image):\n",
        "    result = ocr_model.ocr(image, cls=False)\n",
        "    text = \"\"\n",
        "    for line in result:\n",
        "        text += ' '.join([word_info[1][0] for word_info in line]) + '\\n'\n",
        "    return text\n",
        "\n",
        "def main():\n",
        "    st.title('Organization and Person Name Extractor from Image')\n",
        "    st.write(\"Upload an image containing text to extract organization and person names.\")\n",
        "\n",
        "    # File uploader for images\n",
        "    uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
        "        st.write(\"\")\n",
        "\n",
        "        st.write(\"Extracting text from the image...\")\n",
        "        # Perform OCR on the uploaded image\n",
        "        extracted_text = ocr_image(uploaded_file)\n",
        "\n",
        "        st.write(\"Extracted Text:\")\n",
        "        st.write(extracted_text)\n",
        "\n",
        "        # Extract organization name and person name\n",
        "        organization_name, person_name = extract_organization_and_person_names(extracted_text)\n",
        "\n",
        "        # Define the output directory\n",
        "        output_dir = \"output_directory\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # Define the paths for the output text files\n",
        "        organization_output_path = os.path.join(output_dir, 'organization_name.txt')\n",
        "        person_output_path = os.path.join(output_dir, 'person_name.txt')\n",
        "\n",
        "        # Write the extracted names into text files\n",
        "        with open(organization_output_path, 'w') as f:\n",
        "            f.write(organization_name)\n",
        "\n",
        "        with open(person_output_path, 'w') as f:\n",
        "            f.write(person_name)\n",
        "\n",
        "        # Display the results\n",
        "        st.write(\"### Extracted Information\")\n",
        "        st.write(\"*Extracted Organization Name:*\")\n",
        "        st.write(organization_name)\n",
        "        st.write(\"*Extracted Person Name:*\")\n",
        "        st.write(person_name)\n",
        "\n",
        "        # Provide download links for the text files\n",
        "        st.write(\"### Download Extracted Information\")\n",
        "        st.download_button(label=\"Download Organization Name\", data=open(organization_output_path, 'r').read(), file_name='organization_name.txt')\n",
        "        st.download_button(label=\"Download Person Name\", data=open(person_output_path, 'r').read(), file_name='person_name.txt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHJ57Uez1KxW",
        "outputId": "16f396ee-67fc-4b77-af1d-9538f69a97a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.37.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "[2024/07/27 10:59:58] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-27 11:00:00.528 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# Load spaCy model for organization and person names\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the NER pipeline component for both organization and person names\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "ner.add_label(\"ORG\")\n",
        "ner.add_label(\"PERSON\")\n",
        "\n",
        "# Read data from CSV file for organization names\n",
        "df_org = pd.read_csv(\"/content/Company Names.csv\")  # Update with your CSV file\n",
        "df_person = pd.read_csv(\"/content/Indian_Names.csv\")  # Update with your CSV file for person names\n",
        "\n",
        "# Function to check if a value is a valid string\n",
        "def is_valid_string(value):\n",
        "    return isinstance(value, str) and len(value) > 0\n",
        "\n",
        "# Convert data into spaCy training format for organization names\n",
        "train_examples = []\n",
        "for _, row in df_org.iterrows():\n",
        "    text = row[\"Company Name\"]  # Update with the correct column name\n",
        "    if is_valid_string(text):\n",
        "        annotations = {\"entities\": [(0, len(text), \"ORG\")]}\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "        train_examples.append(example)\n",
        "\n",
        "# Convert data into spaCy training format for person names\n",
        "for _, row in df_person.iterrows():\n",
        "    text = row[\"Name\"]  # Update with the correct column name\n",
        "    if is_valid_string(text):\n",
        "        annotations = {\"entities\": [(0, len(text), \"NAMES\")]}\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "        train_examples.append(example)\n",
        "\n",
        "# Training loop for both organization and person names\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for iteration in range(50):\n",
        "        losses = {}\n",
        "        random.shuffle(train_examples)\n",
        "        for example in train_examples:\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "        print(f\"Iteration {iteration}, Losses: {losses}\")\n",
        "\n",
        "# Save the trained model for both organization and person names\n",
        "nlp.to_disk(\"ner_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WTf9hohZ1LDt",
        "outputId": "f97832df-88b5-42b0-8f6b-f273a4ddabe4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bc463bf6b6aa>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_valid_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NAMES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtrain_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             )\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6mxfT5K1LIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-R1PmmC1LNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
